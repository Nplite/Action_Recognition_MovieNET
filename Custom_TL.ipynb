{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: remotezip in /home/ai/.local/lib/python3.10/site-packages (0.12.3)\n",
            "Requirement already satisfied: requests in /home/ai/.local/lib/python3.10/site-packages (from remotezip) (2.32.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/ai/.local/lib/python3.10/site-packages (from requests->remotezip) (2024.8.30)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ai/.local/lib/python3.10/site-packages (from requests->remotezip) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->remotezip) (3.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->remotezip) (1.26.5)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: tqdm in /home/ai/.local/lib/python3.10/site-packages (4.66.5)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement opencv-python==4.5.2.52 (from versions: 3.4.0.14, 3.4.10.37, 3.4.11.39, 3.4.11.41, 3.4.11.43, 3.4.11.45, 3.4.13.47, 3.4.15.55, 3.4.16.57, 3.4.16.59, 3.4.17.61, 3.4.17.63, 3.4.18.65, 4.3.0.38, 4.4.0.40, 4.4.0.42, 4.4.0.44, 4.4.0.46, 4.5.1.48, 4.5.3.56, 4.5.4.58, 4.5.4.60, 4.5.5.62, 4.5.5.64, 4.6.0.66, 4.7.0.68, 4.7.0.72, 4.8.0.74, 4.8.0.76, 4.8.1.78, 4.9.0.80, 4.10.0.82, 4.10.0.84)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for opencv-python==4.5.2.52\u001b[0m\u001b[31m\n",
            "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement opencv-python-headless==4.5.2.52 (from versions: 3.4.10.37, 3.4.11.39, 3.4.11.41, 3.4.11.43, 3.4.11.45, 3.4.13.47, 3.4.15.55, 3.4.16.59, 3.4.17.61, 3.4.17.63, 3.4.18.65, 4.3.0.38, 4.4.0.40, 4.4.0.42, 4.4.0.44, 4.4.0.46, 4.5.1.48, 4.5.3.56, 4.5.4.58, 4.5.4.60, 4.5.5.62, 4.5.5.64, 4.6.0.66, 4.7.0.68, 4.7.0.72, 4.8.0.74, 4.8.0.76, 4.8.1.78, 4.9.0.80, 4.10.0.82, 4.10.0.84)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for opencv-python-headless==4.5.2.52\u001b[0m\u001b[31m\n",
            "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: tf-models-official in /home/ai/.local/lib/python3.10/site-packages (2.17.0)\n",
            "Requirement already satisfied: sacrebleu in /home/ai/.local/lib/python3.10/site-packages (from tf-models-official) (2.4.3)\n",
            "Requirement already satisfied: py-cpuinfo>=3.3.0 in /home/ai/.local/lib/python3.10/site-packages (from tf-models-official) (9.0.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /home/ai/.local/lib/python3.10/site-packages (from tf-models-official) (1.26.4)\n",
            "Requirement already satisfied: tensorflow-model-optimization>=0.4.1 in /home/ai/.local/lib/python3.10/site-packages (from tf-models-official) (0.8.0)\n",
            "Requirement already satisfied: pycocotools in /home/ai/.local/lib/python3.10/site-packages (from tf-models-official) (2.0)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /home/ai/.local/lib/python3.10/site-packages (from tf-models-official) (6.0.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /home/ai/.local/lib/python3.10/site-packages (from tf-models-official) (0.16.1)\n",
            "Requirement already satisfied: Cython in /home/ai/.local/lib/python3.10/site-packages (from tf-models-official) (3.0.11)\n",
            "Requirement already satisfied: sentencepiece in /home/ai/.local/lib/python3.10/site-packages (from tf-models-official) (0.2.0)\n",
            "Requirement already satisfied: tensorflow~=2.17.0 in /home/ai/.local/lib/python3.10/site-packages (from tf-models-official) (2.17.0)\n",
            "Requirement already satisfied: tf-keras>=2.16.0 in /home/ai/.local/lib/python3.10/site-packages (from tf-models-official) (2.17.0)\n",
            "Requirement already satisfied: tensorflow-text~=2.17.0 in /home/ai/.local/lib/python3.10/site-packages (from tf-models-official) (2.17.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /home/ai/.local/lib/python3.10/site-packages (from tf-models-official) (2.143.0)\n",
            "Requirement already satisfied: matplotlib in /home/ai/.local/lib/python3.10/site-packages (from tf-models-official) (3.9.2)\n",
            "Requirement already satisfied: immutabledict in /home/ai/.local/lib/python3.10/site-packages (from tf-models-official) (4.2.0)\n",
            "Requirement already satisfied: seqeval in /home/ai/.local/lib/python3.10/site-packages (from tf-models-official) (1.2.2)\n",
            "Requirement already satisfied: Pillow in /home/ai/.local/lib/python3.10/site-packages (from tf-models-official) (10.4.0)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /home/ai/.local/lib/python3.10/site-packages (from tf-models-official) (1.6.17)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /home/ai/.local/lib/python3.10/site-packages (from tf-models-official) (6.0.2)\n",
            "Requirement already satisfied: gin-config in /home/ai/.local/lib/python3.10/site-packages (from tf-models-official) (0.5.0)\n",
            "Requirement already satisfied: opencv-python-headless in /home/ai/.local/lib/python3.10/site-packages (from tf-models-official) (3.4.18.65)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /home/ai/.local/lib/python3.10/site-packages (from tf-models-official) (1.14.1)\n",
            "Requirement already satisfied: tf-slim>=1.1.0 in /home/ai/.local/lib/python3.10/site-packages (from tf-models-official) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from tf-models-official) (1.16.0)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /home/ai/.local/lib/python3.10/site-packages (from tf-models-official) (2.2.2)\n",
            "Requirement already satisfied: oauth2client in /home/ai/.local/lib/python3.10/site-packages (from tf-models-official) (4.1.3)\n",
            "Requirement already satisfied: tensorflow-datasets in /home/ai/.local/lib/python3.10/site-packages (from tf-models-official) (4.9.6)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /home/ai/.local/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (2.19.2)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/lib/python3/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.20.2)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /home/ai/.local/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.2.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /home/ai/.local/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (2.34.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /home/ai/.local/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (4.1.1)\n",
            "Requirement already satisfied: bleach in /home/ai/.local/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official) (6.1.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /home/ai/.local/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official) (2024.8.30)\n",
            "Requirement already satisfied: requests in /home/ai/.local/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official) (2.32.3)\n",
            "Requirement already satisfied: python-slugify in /home/ai/.local/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official) (8.0.4)\n",
            "Requirement already satisfied: tqdm in /home/ai/.local/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official) (4.66.5)\n",
            "Requirement already satisfied: python-dateutil in /home/ai/.local/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3 in /usr/lib/python3/dist-packages (from kaggle>=1.3.9->tf-models-official) (1.26.5)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/ai/.local/lib/python3.10/site-packages (from pandas>=0.22.0->tf-models-official) (2024.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=0.22.0->tf-models-official) (2022.1)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /home/ai/.local/lib/python3.10/site-packages (from tensorflow~=2.17.0->tf-models-official) (3.11.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/ai/.local/lib/python3.10/site-packages (from tensorflow~=2.17.0->tf-models-official) (0.37.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/ai/.local/lib/python3.10/site-packages (from tensorflow~=2.17.0->tf-models-official) (3.20.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /home/ai/.local/lib/python3.10/site-packages (from tensorflow~=2.17.0->tf-models-official) (3.3.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /home/ai/.local/lib/python3.10/site-packages (from tensorflow~=2.17.0->tf-models-official) (3.5.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/ai/.local/lib/python3.10/site-packages (from tensorflow~=2.17.0->tf-models-official) (1.65.5)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow~=2.17.0->tf-models-official) (59.6.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /home/ai/.local/lib/python3.10/site-packages (from tensorflow~=2.17.0->tf-models-official) (2.17.1)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/ai/.local/lib/python3.10/site-packages (from tensorflow~=2.17.0->tf-models-official) (0.6.0)\n",
            "Requirement already satisfied: packaging in /home/ai/.local/lib/python3.10/site-packages (from tensorflow~=2.17.0->tf-models-official) (24.1)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /home/ai/.local/lib/python3.10/site-packages (from tensorflow~=2.17.0->tf-models-official) (24.3.25)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /home/ai/.local/lib/python3.10/site-packages (from tensorflow~=2.17.0->tf-models-official) (0.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /home/ai/.local/lib/python3.10/site-packages (from tensorflow~=2.17.0->tf-models-official) (2.4.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /home/ai/.local/lib/python3.10/site-packages (from tensorflow~=2.17.0->tf-models-official) (18.1.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /home/ai/.local/lib/python3.10/site-packages (from tensorflow~=2.17.0->tf-models-official) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /home/ai/.local/lib/python3.10/site-packages (from tensorflow~=2.17.0->tf-models-official) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /home/ai/.local/lib/python3.10/site-packages (from tensorflow~=2.17.0->tf-models-official) (1.16.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /home/ai/.local/lib/python3.10/site-packages (from tensorflow~=2.17.0->tf-models-official) (1.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /home/ai/.local/lib/python3.10/site-packages (from tensorflow~=2.17.0->tf-models-official) (4.12.2)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /home/ai/.local/lib/python3.10/site-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official) (0.1.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->tf-models-official) (2.4.7)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/ai/.local/lib/python3.10/site-packages (from matplotlib->tf-models-official) (4.53.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/ai/.local/lib/python3.10/site-packages (from matplotlib->tf-models-official) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/ai/.local/lib/python3.10/site-packages (from matplotlib->tf-models-official) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ai/.local/lib/python3.10/site-packages (from matplotlib->tf-models-official) (1.4.5)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /home/ai/.local/lib/python3.10/site-packages (from oauth2client->tf-models-official) (0.6.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /home/ai/.local/lib/python3.10/site-packages (from oauth2client->tf-models-official) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /home/ai/.local/lib/python3.10/site-packages (from oauth2client->tf-models-official) (0.4.0)\n",
            "Requirement already satisfied: colorama in /usr/lib/python3/dist-packages (from sacrebleu->tf-models-official) (0.4.4)\n",
            "Requirement already satisfied: regex in /home/ai/.local/lib/python3.10/site-packages (from sacrebleu->tf-models-official) (2024.7.24)\n",
            "Requirement already satisfied: portalocker in /home/ai/.local/lib/python3.10/site-packages (from sacrebleu->tf-models-official) (2.10.1)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /home/ai/.local/lib/python3.10/site-packages (from sacrebleu->tf-models-official) (0.9.0)\n",
            "Requirement already satisfied: lxml in /home/ai/.local/lib/python3.10/site-packages (from sacrebleu->tf-models-official) (5.3.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /home/ai/.local/lib/python3.10/site-packages (from seqeval->tf-models-official) (1.5.1)\n",
            "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from tensorflow-datasets->tf-models-official) (8.0.3)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /home/ai/.local/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official) (0.5.1)\n",
            "Requirement already satisfied: etils[enp,epath,epy,etree]>=1.6.0 in /home/ai/.local/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official) (1.9.4)\n",
            "Requirement already satisfied: pyarrow in /home/ai/.local/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official) (17.0.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /home/ai/.local/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official) (1.15.0)\n",
            "Requirement already satisfied: simple-parsing in /home/ai/.local/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official) (0.1.5)\n",
            "Requirement already satisfied: promise in /home/ai/.local/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official) (2.3)\n",
            "Requirement already satisfied: toml in /home/ai/.local/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official) (0.10.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow~=2.17.0->tf-models-official) (0.37.1)\n",
            "Requirement already satisfied: zipp in /usr/lib/python3/dist-packages (from etils[enp,epath,epy,etree]>=1.6.0->tensorflow-datasets->tf-models-official) (1.0.0)\n",
            "Requirement already satisfied: fsspec in /home/ai/.local/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0->tensorflow-datasets->tf-models-official) (2024.6.1)\n",
            "Requirement already satisfied: importlib_resources in /home/ai/.local/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0->tensorflow-datasets->tf-models-official) (6.4.3)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /home/ai/.local/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official) (1.24.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /home/ai/.local/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official) (1.65.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ai/.local/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=1.6.7->tf-models-official) (5.5.0)\n",
            "Requirement already satisfied: rich in /home/ai/.local/lib/python3.10/site-packages (from keras>=3.2.0->tensorflow~=2.17.0->tf-models-official) (13.7.1)\n",
            "Requirement already satisfied: optree in /home/ai/.local/lib/python3.10/site-packages (from keras>=3.2.0->tensorflow~=2.17.0->tf-models-official) (0.12.1)\n",
            "Requirement already satisfied: namex in /home/ai/.local/lib/python3.10/site-packages (from keras>=3.2.0->tensorflow~=2.17.0->tf-models-official) (0.0.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ai/.local/lib/python3.10/site-packages (from requests->kaggle>=1.3.9->tf-models-official) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->kaggle>=1.3.9->tf-models-official) (3.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ai/.local/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (3.5.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /home/ai/.local/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (1.4.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /home/ai/.local/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow~=2.17.0->tf-models-official) (3.0.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/ai/.local/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow~=2.17.0->tf-models-official) (0.7.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /home/ai/.local/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow~=2.17.0->tf-models-official) (3.7)\n",
            "Requirement already satisfied: webencodings in /home/ai/.local/lib/python3.10/site-packages (from bleach->kaggle>=1.3.9->tf-models-official) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /home/ai/.local/lib/python3.10/site-packages (from python-slugify->kaggle>=1.3.9->tf-models-official) (1.3)\n",
            "Requirement already satisfied: docstring-parser~=0.15 in /home/ai/.local/lib/python3.10/site-packages (from simple-parsing->tensorflow-datasets->tf-models-official) (0.16)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ai/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow~=2.17.0->tf-models-official) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ai/.local/lib/python3.10/site-packages (from rich->keras>=3.2.0->tensorflow~=2.17.0->tf-models-official) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ai/.local/lib/python3.10/site-packages (from rich->keras>=3.2.0->tensorflow~=2.17.0->tf-models-official) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/ai/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow~=2.17.0->tf-models-official) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install remotezip \n",
        "!pip install tqdm \n",
        "!pip install opencv-python==4.5.2.52\n",
        "!pip install opencv-python-headless==4.5.2.52 \n",
        "!pip install tf-models-official"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QImPsudoK9JI"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-09-05 11:01:15.583157: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-09-05 11:01:15.727701: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-05 11:01:15.783781: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-05 11:01:15.798732: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-05 11:01:15.899221: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-09-05 11:01:16.785096: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import tqdm\n",
        "import random\n",
        "import pathlib\n",
        "import itertools\n",
        "import collections\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import remotezip as rz\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "# Import the MoViNet model from TensorFlow Models (tf-models-official) for the MoViNet model\n",
        "from official.projects.movinet.modeling import movinet\n",
        "from official.projects.movinet.modeling import movinet_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "fwEhJ13_PSy6"
      },
      "outputs": [],
      "source": [
        "def format_frames(frame, output_size):\n",
        "  frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
        "  frame = tf.image.resize_with_pad(frame, *output_size)\n",
        "  return frame\n",
        "\n",
        "def frames_from_video_file(video_path, n_frames, output_size = (224,224), frame_step = 15):\n",
        "  # Read each video frame by frame\n",
        "  result = []\n",
        "  src = cv2.VideoCapture(str(video_path))  \n",
        "\n",
        "  video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "\n",
        "  need_length = 1 + (n_frames - 1) * frame_step\n",
        "\n",
        "  if need_length > video_length:\n",
        "    start = 0\n",
        "  else:\n",
        "    max_start = video_length - need_length\n",
        "    start = random.randint(0, max_start + 1)\n",
        "\n",
        "  src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
        "  # ret is a boolean indicating whether read was successful, frame is the image itself\n",
        "  ret, frame = src.read()\n",
        "  result.append(format_frames(frame, output_size))\n",
        "\n",
        "  for _ in range(n_frames - 1):\n",
        "    for _ in range(frame_step):\n",
        "      ret, frame = src.read()\n",
        "    if ret:\n",
        "      frame = format_frames(frame, output_size)\n",
        "      result.append(frame)\n",
        "    else:\n",
        "      result.append(np.zeros_like(result[0]))\n",
        "  src.release()\n",
        "  result = np.array(result)[..., [2, 1, 0]]\n",
        "\n",
        "  return result\n",
        "\n",
        "class FrameGenerator:\n",
        "  def __init__(self, path, n_frames, training = False):\n",
        "    self.path = path\n",
        "    self.n_frames = n_frames\n",
        "    self.training = training\n",
        "    self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n",
        "    self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n",
        "\n",
        "  def get_files_and_class_names(self):\n",
        "    video_paths = list(self.path.glob('*/*.avi'))\n",
        "    classes = [p.parent.name for p in video_paths] \n",
        "    return video_paths, classes\n",
        "\n",
        "  def __call__(self):\n",
        "    video_paths, classes = self.get_files_and_class_names()\n",
        "\n",
        "    pairs = list(zip(video_paths, classes))\n",
        "\n",
        "    if self.training:\n",
        "      random.shuffle(pairs)\n",
        "\n",
        "    for path, name in pairs:\n",
        "      video_frames = frames_from_video_file(path, self.n_frames) \n",
        "      label = self.class_ids_for_name[name] # Encode labels\n",
        "      yield video_frames, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYYShfhMx9DW"
      },
      "source": [
        "Create the training and test datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1725514284.321059   19186 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-09-05 11:01:24.425584: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "batch_size = 8\n",
        "num_frames = 8\n",
        "\n",
        "# Define the subset paths based on your downloaded dataset\n",
        "subset_paths = {\n",
        "    'train': Path('dataset/train'),\n",
        "    'test': Path('dataset/test')\n",
        "}\n",
        "\n",
        "output_signature = (\n",
        "    tf.TensorSpec(shape=(None, None, None, 3), dtype=tf.float32),\n",
        "    tf.TensorSpec(shape=(), dtype=tf.int16)\n",
        ")\n",
        "\n",
        "train_ds = tf.data.Dataset.from_generator(\n",
        "    FrameGenerator(subset_paths['train'], num_frames, training=True),\n",
        "    output_signature=output_signature\n",
        ").batch(batch_size)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_generator(\n",
        "    FrameGenerator(subset_paths['test'], num_frames),\n",
        "    output_signature=output_signature\n",
        ").batch(batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Labels saved to label.txt\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "batch_size = 8\n",
        "num_frames = 8\n",
        "\n",
        "# Define the subset paths based on your downloaded dataset\n",
        "subset_paths = {\n",
        "    'train': Path('dataset/train'),\n",
        "    'test': Path('dataset/test')\n",
        "}\n",
        "\n",
        "output_signature = (\n",
        "    tf.TensorSpec(shape=(None, None, None, 3), dtype=tf.float32),\n",
        "    tf.TensorSpec(shape=(), dtype=tf.int16)\n",
        ")\n",
        "\n",
        "train_ds = tf.data.Dataset.from_generator(\n",
        "    FrameGenerator(subset_paths['train'], num_frames, training=True),\n",
        "    output_signature=output_signature\n",
        ").batch(batch_size)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_generator(\n",
        "    FrameGenerator(subset_paths['test'], num_frames),\n",
        "    output_signature=output_signature\n",
        ").batch(batch_size)\n",
        "\n",
        "# Collect unique labels from the dataset\n",
        "unique_labels = set()\n",
        "for frames, labels in train_ds:\n",
        "    unique_labels.update(labels.numpy())\n",
        "\n",
        "# Sort the labels for consistency (optional)\n",
        "unique_labels = sorted(unique_labels)\n",
        "\n",
        "# Save the labels to a label.txt file\n",
        "label_file_path = Path('label.txt')\n",
        "with label_file_path.open('w') as f:\n",
        "    for label in unique_labels:\n",
        "        f.write(f\"{label}\\n\")\n",
        "\n",
        "print(f\"Labels saved to {label_file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "k9L2-toXCOQq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([1 2 1 1 1 2 0 0], shape=(8,), dtype=int16)\n",
            "tf.Tensor([0 2 2 0 1 2 2 2], shape=(8,), dtype=int16)\n",
            "tf.Tensor([2 2 2 0 0 1 2 1], shape=(8,), dtype=int16)\n",
            "tf.Tensor([2 2 0 0 1 0 2 1], shape=(8,), dtype=int16)\n",
            "tf.Tensor([0 2 0 0 1 2 1 0], shape=(8,), dtype=int16)\n",
            "tf.Tensor([0 0 0 2 2 1 0 1], shape=(8,), dtype=int16)\n",
            "tf.Tensor([0 0 1 1 2 1 1 0], shape=(8,), dtype=int16)\n",
            "tf.Tensor([0 2 2 1 1 1 0 0], shape=(8,), dtype=int16)\n",
            "tf.Tensor([2 2 1 0 0 1 2 1], shape=(8,), dtype=int16)\n",
            "tf.Tensor([2 0 1 0 2 1 1 1], shape=(8,), dtype=int16)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-09-05 11:02:09.361144: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
          ]
        }
      ],
      "source": [
        "for frames, labels in train_ds.take(10):\n",
        "  print(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ3qwZnpfy9c"
      },
      "source": [
        "Take a look at the shape of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['0' '1' '2']\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pathlib\n",
        "\n",
        "def load_labels():\n",
        "    \"\"\"Load the custom labels from the local label.txt file.\"\"\"\n",
        "    labels_path = pathlib.Path('label.txt')  # Path to your custom label.txt file\n",
        "    lines = labels_path.read_text().splitlines()  # Read the file and split it into lines\n",
        "    return np.array([line.strip() for line in lines])  # Convert lines to a NumPy array\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "b6MqP4m2fyQT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape: (8, 8, 224, 224, 3)\n",
            "Label: (8,)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Shape: {frames.shape}\")\n",
        "print(f\"Label: {labels.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dMvDkgfFZC6a"
      },
      "outputs": [],
      "source": [
        "gru = layers.GRU(units=4, return_sequences=True, return_state=True)\n",
        "\n",
        "inputs = tf.random.normal(shape=[1, 10, 8]) # (batch, sequence, channels)\n",
        "\n",
        "result, state = gru(inputs) # Run it all at once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bI8FOPRRXXPa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "first_half, state = gru(inputs[:, :5, :])   # run the first half, and capture the state\n",
        "second_half, _ = gru(inputs[:,5:, :], initial_state=state)  # Use the state to continue where you left off.\n",
        "\n",
        "print(np.allclose(result[:, :5,:], first_half))\n",
        "print(np.allclose(result[:, 5:,:], second_half))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "rhSCM6cee05F"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "movinet_a0_base/\n",
            "movinet_a0_base/checkpoint\n",
            "movinet_a0_base/ckpt-1.data-00000-of-00001\n",
            "movinet_a0_base/ckpt-1.index\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x77a1638531c0>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_id = 'a0'\n",
        "resolution = 224\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "backbone = movinet.Movinet(model_id=model_id)\n",
        "backbone.trainable = False\n",
        "\n",
        "# Set num_classes=600 to load the pre-trained weights from the original model  \n",
        "model = movinet_model.MovinetClassifier(backbone=backbone, num_classes=600)\n",
        "model.build([None, None, None, None, 3])\n",
        "\n",
        "# Load pre-trained weights\n",
        "!wget https://storage.googleapis.com/tf_model_garden/vision/movinet/movinet_a0_base.tar.gz -O movinet_a0_base.tar.gz -q\n",
        "!tar -xvf movinet_a0_base.tar.gz\n",
        "\n",
        "checkpoint_dir = f'movinet_{model_id}_base'\n",
        "checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "checkpoint = tf.train.Checkpoint(model=model)\n",
        "status = checkpoint.restore(checkpoint_path)\n",
        "status.assert_existing_objects_matched()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6cfAelbU5Gi3"
      },
      "outputs": [],
      "source": [
        "def build_classifier(batch_size, num_frames, resolution, backbone, num_classes):\n",
        "  \"\"\"Builds a classifier on top of a backbone model.\"\"\"\n",
        "  model = movinet_model.MovinetClassifier(\n",
        "      backbone=backbone,\n",
        "      num_classes=num_classes)\n",
        "  model.build([batch_size, num_frames, resolution, resolution, 3])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9HWSk-u7oPUZ"
      },
      "outputs": [],
      "source": [
        "model = build_classifier(batch_size, num_frames, resolution, backbone, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "dVqBLrn1tBsd"
      },
      "outputs": [],
      "source": [
        "num_epochs = 5\n",
        "loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "model.compile(loss=loss_obj, optimizer='adam', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9ZeiYzI0tqQG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1725449354.024196   65996 service.cc:146] XLA service 0x779fb0029c60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "I0000 00:00:1725449354.024220   65996 service.cc:154]   StreamExecutor device (0): Host, Default Version\n",
            "I0000 00:00:1725449354.033781   65996 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      1/Unknown - 12s 12s/step - loss: 2.3192 - accuracy: 0.0000e+00"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-09-04 16:59:20.835865: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "38/38 [==============================] - 313s 8s/step - loss: 0.5215 - accuracy: 0.8800 - val_loss: 0.0457 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-09-04 17:04:21.681861: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "\t [[{{node IteratorGetNext}}]]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "38/38 [==============================] - 302s 8s/step - loss: 0.0662 - accuracy: 0.9767 - val_loss: 0.0340 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "38/38 [==============================] - 300s 8s/step - loss: 0.0295 - accuracy: 0.9967 - val_loss: 0.0195 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "38/38 [==============================] - 298s 8s/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.0168 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "38/38 [==============================] - 298s 8s/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0169 - val_accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "results = model.fit(train_ds,\n",
        "                    validation_data=test_ds,\n",
        "                    epochs=num_epochs,\n",
        "                    validation_freq=1,\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "NqgbzOiKuxxT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11/11 [==============================] - 67s 6s/step - loss: 0.0195 - accuracy: 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-09-04 17:30:09.366744: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "\t [[{{node IteratorGetNext}}]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'loss': 0.01945176161825657, 'accuracy': 1.0}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(test_ds, return_dict=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved successfully...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ai/.local/lib/python3.10/site-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model.save(\"Movinet_Model.h5\")\n",
        "print(\"Model saved successfully...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[ WARN:0@2992.657] VIDEOIO(V4L2:/dev/video1): can't open camera by index\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Unknown config_item: 'Vision>Movinet'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[35], line 63\u001b[0m\n\u001b[1;32m     59\u001b[0m     cap\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m     60\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m---> 63\u001b[0m \u001b[43mtest_model_with_webcam\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMovinet_Model.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[35], line 18\u001b[0m, in \u001b[0;36mtest_model_with_webcam\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Load the saved model with custom objects\u001b[39;00m\n\u001b[1;32m     17\u001b[0m custom_objects \u001b[38;5;241m=\u001b[39m get_custom_objects()\n\u001b[0;32m---> 18\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: Could not open webcam.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/saving/saving_api.py:194\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    188\u001b[0m         filepath,\n\u001b[1;32m    189\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[1;32m    191\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[1;32m    192\u001b[0m     )\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_h5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_from_hdf5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    200\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    202\u001b[0m     )\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/legacy/saving/legacy_h5_format.py:133\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    130\u001b[0m model_config \u001b[38;5;241m=\u001b[39m json_utils\u001b[38;5;241m.\u001b[39mdecode(model_config)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m saving_options\u001b[38;5;241m.\u001b[39mkeras_option_scope(use_legacy_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 133\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43msaving_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# set weights\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     load_weights_from_hdf5_group(f[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m], model)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/legacy/saving/saving_utils.py:85\u001b[0m, in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# TODO(nkovela): Swap find and replace args during Keras 3.0 release\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Replace keras refs with keras\u001b[39;00m\n\u001b[1;32m     83\u001b[0m config \u001b[38;5;241m=\u001b[39m _find_replace_nested_dict(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mserialization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODULE_OBJECTS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL_OBJECTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprintable_module_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlayer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/legacy/saving/serialization.py:473\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(identifier, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;66;03m# In this case we are dealing with a Keras config dictionary.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m     config \u001b[38;5;241m=\u001b[39m identifier\n\u001b[0;32m--> 473\u001b[0m     (\u001b[38;5;28mcls\u001b[39m, cls_config) \u001b[38;5;241m=\u001b[39m \u001b[43mclass_and_config_for_serialized_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprintable_module_name\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;66;03m# If this object has already been loaded (i.e. it's shared between\u001b[39;00m\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;66;03m# multiple objects), return the already-loaded object.\u001b[39;00m\n\u001b[1;32m    479\u001b[0m     shared_object_id \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(SHARED_OBJECT_KEY)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/legacy/saving/serialization.py:381\u001b[0m, in \u001b[0;36mclass_and_config_for_serialized_keras_object\u001b[0;34m(config, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    379\u001b[0m     deserialized_objects[key] \u001b[38;5;241m=\u001b[39m item\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__passive_serialization__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m item:\n\u001b[0;32m--> 381\u001b[0m     deserialized_objects[key] \u001b[38;5;241m=\u001b[39m \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprintable_module_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig_item\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;66;03m# TODO(momernick): Should this also have 'module_objects'?\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misfunction(\n\u001b[1;32m    389\u001b[0m     object_registration\u001b[38;5;241m.\u001b[39mget_registered_object(item, custom_objects)\n\u001b[1;32m    390\u001b[0m ):\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;66;03m# naming conflict with a custom object, since the config of an\u001b[39;00m\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;66;03m# object will always be a dict.\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/legacy/saving/serialization.py:473\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(identifier, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;66;03m# In this case we are dealing with a Keras config dictionary.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m     config \u001b[38;5;241m=\u001b[39m identifier\n\u001b[0;32m--> 473\u001b[0m     (\u001b[38;5;28mcls\u001b[39m, cls_config) \u001b[38;5;241m=\u001b[39m \u001b[43mclass_and_config_for_serialized_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprintable_module_name\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;66;03m# If this object has already been loaded (i.e. it's shared between\u001b[39;00m\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;66;03m# multiple objects), return the already-loaded object.\u001b[39;00m\n\u001b[1;32m    479\u001b[0m     shared_object_id \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(SHARED_OBJECT_KEY)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/legacy/saving/serialization.py:354\u001b[0m, in \u001b[0;36mclass_and_config_for_serialized_keras_object\u001b[0;34m(config, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m object_registration\u001b[38;5;241m.\u001b[39mget_registered_object(\n\u001b[1;32m    351\u001b[0m     class_name, custom_objects, module_objects\n\u001b[1;32m    352\u001b[0m )\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    355\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprintable_module_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    356\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure you are using a `keras.utils.custom_object_scope` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    357\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand that this object is included in the scope. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    358\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.tensorflow.org/guide/keras/save_and_serialize\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#registering_the_custom_object for details.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m     )\n\u001b[1;32m    362\u001b[0m cls_config \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# Check if `cls_config` is a list. If it is a list, return the class and the\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# associated class configs for recursively deserialization. This case will\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;66;03m# happen on the old version of sequential model (e.g. `keras_version` ==\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;66;03m# \"2.0.6\"), which is serialized in a different structure, for example\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;66;03m# \"{'class_name': 'Sequential',\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m#   'config': [{'class_name': 'Embedding', 'config': ...}, {}, ...]}\".\u001b[39;00m\n",
            "\u001b[0;31mValueError\u001b[0m: Unknown config_item: 'Vision>Movinet'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details."
          ]
        }
      ],
      "source": [
        "\n",
        "def test_model_with_webcam(model_path):\n",
        "    def format_frames(frame, output_size):\n",
        "        frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
        "        frame = tf.image.resize_with_pad(frame, *output_size)\n",
        "        return frame\n",
        "\n",
        "    def get_custom_objects():\n",
        "        return {\n",
        "            'MovinetClassifier': movinet_model.MovinetClassifier\n",
        "        }\n",
        "\n",
        "    num_frames = 8\n",
        "    output_size = (224, 224)\n",
        "    cap = cv2.VideoCapture(1)\n",
        "\n",
        "    # Load the saved model with custom objects\n",
        "    custom_objects = get_custom_objects()\n",
        "    model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open webcam.\")\n",
        "        return\n",
        "\n",
        "    frame_buffer = []\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if not ret:\n",
        "            print(\"Error: Failed to capture frame.\")\n",
        "            break\n",
        "\n",
        "        # Preprocess the frame\n",
        "        processed_frame = format_frames(frame, output_size)\n",
        "        frame_buffer.append(processed_frame)\n",
        "\n",
        "        if len(frame_buffer) == num_frames:\n",
        "            # Stack frames into a batch\n",
        "            input_frames = np.expand_dims(frame_buffer, axis=0)\n",
        "\n",
        "            # Make predictions\n",
        "            predictions = model.predict(input_frames)\n",
        "            predicted_class = np.argmax(predictions[0])\n",
        "\n",
        "            # Display the prediction on the frame\n",
        "            label = f\"Predicted: {predicted_class}\"\n",
        "            cv2.putText(frame, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
        "\n",
        "            # Clear the frame buffer\n",
        "            frame_buffer = []\n",
        "\n",
        "        # Show the frame with predictions\n",
        "        cv2.imshow(\"Webcam\", frame)\n",
        "\n",
        "        # Break the loop on 'q' key press\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "test_model_with_webcam(\"Movinet_Model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "transfer_learning_with_movinet.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
